{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining [EN.550.436]\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "**Class 12** - Oct 17, 2016\n",
    "\n",
    "- Classification exercises\n",
    "- Cross-validation\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Classification</font></h1>\n",
    "\n",
    "- Based on a **training set** of labeled points, assign class labels to unknown vectors in the **query set**.  \n",
    "\n",
    "> **Training set**\n",
    "\n",
    ">$T = \\big\\{ (x_i, C_i) \\big\\}$ \n",
    "\n",
    "> where $x_i\\in \\mathbb{R}^d$ are feature sets and $C_i$ are the known class memberships\n",
    "\n",
    "<nbsp/>\n",
    "\n",
    "> **Query set**\n",
    "\n",
    ">$Q = \\big\\{ x_i \\big\\}$ where $x_i\\in \\mathbb{R}^d$ consist of the kind of features in $T$\n",
    "\n",
    "- And again, $x_i$ are not real vectors but **feature sets** of a bunch of scalars in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes with Covariance Matrix\n",
    "\n",
    "- Estimate the full covariance matrix for the classes\n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) =  G(\\boldsymbol{x};\\mu_k, \\Sigma_k)$\n",
    "\n",
    "> Handles correlated features well\n",
    "\n",
    "- Consider binary problem with 2 classes\n",
    "\n",
    "> Taking the negative logarithm of the likelihoods we compare\n",
    "\n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma_1^{-1}(x\\!-\\!\\mu_1) + \\ln|\\Sigma_1|$ vs.\n",
    "\n",
    ">$\\displaystyle (x\\!-\\!\\mu_2)^T\\,\\Sigma_2^{-1}(x\\!-\\!\\mu_2) + \\ln|\\Sigma_2|$\n",
    "\n",
    "> If the difference is lower than a threshold, we classify it accordingly\n",
    "\n",
    "- This is called **Quadratic Discriminant Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Covariance Matrix\n",
    "\n",
    "- When $\\Sigma_1=\\Sigma_2=\\Sigma$, the quadratic terms cancel from the difference\n",
    " \n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_1) $ \n",
    ">$\\displaystyle -\\ (x\\!-\\!\\mu_2)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_2) $\n",
    "\n",
    "- Hence this is called **Linear Discriminant Analysis**\n",
    "\n",
    "> Fewer parameters to estimate during the learning process\n",
    "\n",
    "> Good, if we don't have enough data, for example...\n",
    "\n",
    "> Think linear vs quadratic fitting and how you decide between those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: QDA \n",
    "\n",
    "- Use the provided [training](files/Class12-Train.csv) and [query](files/Class12-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n",
    "- Place post-it on your laptop to indicate your status and if you need help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "#necessary step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyQDA(object):\n",
    "    \"\"\" Template for classifier\n",
    "    \"\"\"\n",
    "    def fit(self,X,C):\n",
    "        # your code here\n",
    "        return self\n",
    "\n",
    "    def predict(self,Y):\n",
    "        Cpred = None\n",
    "        # your code here\n",
    "        # use linalg.det(matrix)\n",
    "        # and linalg.inv(matrix)\n",
    "        return Cpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyQDA(dict):\n",
    "    \"\"\" Simple implementation for illustration purposes\n",
    "    \"\"\"       \n",
    "    def fit(self,X,C):#一定要有self\n",
    "        for k in np.unique(C):   #找不同的类\n",
    "            members = (C==k)\n",
    "            prior = members.sum() / float(C.size)#防止得到整数\n",
    "            S = X[members,:] # subset of class \n",
    "            mu = S.mean(axis=0)    #列向量求均值\n",
    "            Z = (S-mu).T # centered column vectors->row vectors\n",
    "            cov = Z.dot(Z.T) / (Z[0,:].size-1)  #row.dot(column)\n",
    "            self[k] = (mu,cov,prior)\n",
    "        return self\n",
    "            \n",
    "    def predict(self,Y):\n",
    "        Cpred = -1 * ones(Y[:,0].size)\n",
    "        for i in range(Cpred.size):\n",
    "            d2min, kbest = 1e99, None\n",
    "            for k in self:\n",
    "                mu, cov, prior = self[k]\n",
    "                diff = (Y[i,:]-mu).T\n",
    "                d2 = diff.T.dot(linalg.inv(cov)).dot(diff) / 2\n",
    "                d2 += np.log(linalg.det(cov)) / 2 - np.log(prior) \n",
    "                if d2<d2min: d2min,kbest = d2,k\n",
    "            Cpred[i] = kbest\n",
    "        return Cpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different estimates: 0\n"
     ]
    }
   ],
   "source": [
    "# reference implementation\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "Q = np.loadtxt('files/Class12-Query.csv', delimiter=',')\n",
    "D = np.loadtxt('files/Class12-Train.csv', delimiter=',')\n",
    "X, C = D[:,0:2], D[:,2]\n",
    "\n",
    "Cpred = MyQDA().fit(X,C).predict(Q)\n",
    "Cskit =   QDA().fit(X,C).predict(Q)\n",
    "\n",
    "print 'Number of different estimates:', (Cpred!=Cskit).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Cross-Validation</font></h1>\n",
    "\n",
    "- How to evaluate the quality of estimator?\n",
    "\n",
    "> $k$-NN method's parameter affects the results\n",
    "\n",
    "- We saw on the IRIS data that 1-NN was overfitting\n",
    "\n",
    "> We discussed excluding the point itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions of the Training set\n",
    "\n",
    "- Random complementary subsets \n",
    "\n",
    "> Train on a larger subset, test on a small\n",
    "\n",
    "> Multiple rounds to decrease variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out\n",
    "\n",
    "- For each point, we train on the others and test\n",
    "\n",
    "> Testing on $n$ points requires $n$ training \n",
    "\n",
    "- Expensive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Relaxed Variant\n",
    "\n",
    "- $k$-fold cross-validation \n",
    "\n",
    "> 0. Create $k$ partitions of equal sizes, e.g., $k=2$ yields two subsets\n",
    "> 0. Pick a single partition and train on the other $(k\\!-\\!1)$ \n",
    "> 0. Repeat for all $k$ partitions - requires $k$ trainings\n",
    "\n",
    "- Leave-One-Out is a special case with $k=n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cross-Validation\n",
    "\n",
    "- Evaluate \"QDA\" on the [training](files/Class12-Train.csv) set using 2-fold cross-validation\n",
    "\n",
    ">0. What is the fraction of correct estimates? \n",
    ">0. What is the uncertainty of that fraction?\n",
    " \n",
    "> The **training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case #1 - Number of mislabeled points out of a total 157 points : 23\n",
      "Case #2 - Number of mislabeled points out of a total 156 points : 15\n"
     ]
    }
   ],
   "source": [
    "# randomize and split to D1 + D2\n",
    "np.random.shuffle(D)\n",
    "split = D[:,0].size / 2\n",
    "D1, D2 = D[:split,:], D[split:,:]\n",
    "# train on one estimate on the other\n",
    "for (i,T,Q) in [(1,D1,D2),(2,D2,D1)]:\n",
    "    X, C = T[:,0:2], T[:,2]\n",
    "    Cpred,Ctrue= MyQDA().fit(X,C).predict(Q[:,:2]),Q[:,2]\n",
    "    print \"Case #%d - Number of mislabeled points out of a total %3d points : %2d\" \\\n",
    "        % (i, Q.shape[0],(Ctrue!=Cpred).sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done already?\n",
    "\n",
    "- Visualize the results in the 2D features space\n",
    "- Make these simple codes run faster \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "- Implement LDA and compare to sklearn\n",
    "\n",
    ">0. Write code without using sklearn \n",
    ">0. Apply to [training](files/Class12-Train.csv) and [query](files/Class12-Query.csv) sets \n",
    ">0. Compare your results to sklearn's \n",
    "\n",
    "- Perform 10-fold cross-validation of *MyQDA* on [this](files/Class12-Train.csv) file\n",
    "\n",
    ">0. Write code without using sklearn \n",
    ">0. Calculate average and variance of good classifications \n",
    ">0. Compare to sklearn "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
