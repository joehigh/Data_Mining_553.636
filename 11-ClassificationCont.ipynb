{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining [EN.550.436]\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "**Class 11** - Oct 12, 2016\n",
    "\n",
    "- Classification problems\n",
    "- Naive Bayes Classifier\n",
    "- Linear Discriminant Analysis\n",
    "- Quadratic Discriminant Analysis\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Classification</font></h1>\n",
    "\n",
    "- Based on a **training set** of labeled points, assign class labels to unknown vectors in the **query set**.  \n",
    "\n",
    "> **Training set**\n",
    "\n",
    ">$T = \\big\\{ (\\boldsymbol{x}_i, C_i) \\big\\}_{i=1}^N$ where $x_i\\in \\mathbb{R}^d$ and $C_i$ is the known class membership \n",
    "\n",
    "> **Query set**\n",
    "\n",
    ">$Q = \\big\\{ \\boldsymbol{x}_j \\big\\}_{j=1}^M$ where $x_j\\in \\mathbb{R}^d$ \n",
    "\n",
    "> For example,\n",
    "> blood test results ($\\boldsymbol{x}$) and sick/healty ($C$) - we want to predict if a new patient is sick based on the available measurements\n",
    "\n",
    "- Similar to regression but with discrete categories to classify into..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Methods\n",
    "\n",
    "- $k$-NN\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis\n",
    "- Logistic regression\n",
    "- Decisions trees\n",
    "- Random forests\n",
    "- Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Iris Dataset\n",
    "\n",
    "We'll use this data set available in [scikit-learn](http://scikit-learn.org/stable/index.html), see [this](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) page for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples\n",
    "\n",
    "More [exercises](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#examples-using-sklearn-neighbors-kneighborsclassifier) such as classification of [digits](http://scikit-learn.org/stable/auto_examples/exercises/digits_classification_exercise.html) are available at http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "- Using Bayes' rule to infer discrete classes $C_k$ for a given $\\boldsymbol{x}$ set of features\n",
    "\n",
    ">$\\displaystyle P(C_k|\\boldsymbol{x}) = \\frac{1}{Z}\\ \\pi(C_k)\\,{\\cal{}L}_{\\boldsymbol{x}}(C_k)$ \n",
    "\n",
    "\n",
    "- Naively assuming the features are independent \n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) = \\prod_{\\alpha}^d p(x_{\\alpha}|C_k)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Learning\n",
    "\n",
    "- Say for Gaussian likelihoods, we simply estimate the sample mean and variance of all features for each class $k$\n",
    "\n",
    ">$\\displaystyle p(x_{\\alpha}|C_k) = G(x_{\\alpha};\\mu_k, \\sigma^2_k)$\n",
    "\n",
    "- We have to also pick some prior for the classes\n",
    "\n",
    "> Using uniform or based on frequency of points in training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Estimation\n",
    "\n",
    "- Look for maximum of the posterior\n",
    "\n",
    "\n",
    ">$\\displaystyle \\hat{k} =  \\mathrm{arg}\\max_k \\left[ \\pi_k \\prod_{\\alpha}^d G(x_{\\alpha};\\mu_k, \\sigma^2_k)\\right]$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()   #load iris\n",
    "\n",
    "#实际操作中，先读取数据为矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 classes: [0 1 2]\n",
      "0 [ 5.006  3.418  1.464  0.244] [ 0.12424898  0.14517959  0.03010612  0.01149388]\n",
      "1 [ 5.936  2.77   4.26   1.326] [ 0.26643265  0.09846939  0.22081633  0.03910612]\n",
      "2 [ 6.588  2.974  5.552  2.026] [ 0.40434286  0.10400408  0.30458776  0.07543265]\n"
     ]
    }
   ],
   "source": [
    "# unique known classes in training set\n",
    "classes = np.unique(iris.target)   #获取不同类别\n",
    "print 'There are %d classes:' % len(classes), classes\n",
    "\n",
    "# calc feature means and variances for each class\n",
    "param = dict() # we save them in this dictionary，数据类别\n",
    "for k in classes:\n",
    "    members = (iris.target == k) # boolean array，判断每个样本是否属于k\n",
    "    num = members.sum()    # True:1, False:0，类别成员总计\n",
    "    prior = num / float(iris.target.size)   #prior\n",
    "    X = iris.data[members,:] # slice out members，取出第k类数据\n",
    "    mu = X.mean(axis=0)      # calc mean\n",
    "    X -= mu   #center k type\n",
    "    var = (X*X).sum(axis=0) / (X[:,0].size-1)   #X*X对应元素相乘，得到与X一致形状的矩阵,修改为第0列\n",
    "    param[k] = (num, prior, mu, var) # save results\n",
    "    if True: print k, mu, var   #print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.58532504,  0.00324987,  0.33537789,  0.33855365])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init predicted values\n",
    "k_pred = -1 * ones(iris.target.size)\n",
    "\n",
    "# evaluate posterior for each point and find maximum\n",
    "for i in range(iris.target.size):   #遍历每个个体\n",
    "    pmax, kmax = -1, None   # initialize to nonsense values\n",
    "    for k in classes:\n",
    "        num, prior, mu, var = param[k]\n",
    "        diff = iris.data[i,:] - mu\n",
    "        d2 = diff*diff / var / 2    #d2=diff^2/(2*var)\n",
    "        p = prior * np.exp(-d2.sum())   #posterior Guassian,都加起来是认为4个特征是4个独立变量,故直接乘起来等于概率\n",
    "        if p > pmax:   #diff越大,p越小,取得的实际上是最接近的类别\n",
    "            pmax = p\n",
    "            kmax = k\n",
    "    k_pred[i] = kmax\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=k_pred).sum()))   #用已知的数据检验某种算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "# run sklearn's version - read up on differences if interested\n",
    "from sklearn.naive_bayes import GaussianNB   #另一个工具箱,naive bayes，GUASSIAN\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"   #算法肯定有修正\n",
    "      % (iris.target.size, (iris.target!=y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "\n",
    "- Features are automatically treated correctly relative to each other\n",
    "\n",
    "> For example, measuring similar things in different units? 1m vs 1mm\n",
    "\n",
    "> The estimated mean and variance puts them on a meaningful scale\n",
    "\n",
    "- Independence is a strong assumption and for no good reason\n",
    "\n",
    "> Actually it helps with the computational costs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Naive Bayes\n",
    "\n",
    "- Use the provided [training](files/Class12-Train.csv) and [query](files/Class12-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n",
    "- Place post-it on your laptop to indicate your status and if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Covariance Matrix\n",
    "\n",
    "- Estimate the full covariance matrix for the classes\n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) =  G(\\boldsymbol{x};\\mu_k, \\Sigma_k)$\n",
    "\n",
    "> Handles correlated features well\n",
    "\n",
    "- Consider binary problem with 2 classes\n",
    "\n",
    "> Taking the negative logarithm of the likelihoods we compare\n",
    "\n",
    ">$\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1)^T\\,\\Sigma_1^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1) + \\ln|\\Sigma_1|$ vs.\n",
    "\n",
    ">$\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2)^T\\,\\Sigma_2^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2) + \\ln|\\Sigma_2|$\n",
    "\n",
    "> If the difference is lower than a threshold, we classify it accordingly\n",
    "\n",
    "- This is called **Quadratic Discriminant Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Covariance Matrix\n",
    "\n",
    "- When $\\Sigma_1=\\Sigma_2=\\Sigma$, the quadratic terms cancel from the difference\n",
    " \n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_1) $ \n",
    ">$\\displaystyle -\\ (x\\!-\\!\\mu_2)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_2) $\n",
    "\n",
    "- Hence this is called **Linear Discriminant Analysis**\n",
    "\n",
    "> Fewer parameters to estimate during the learning process\n",
    "\n",
    "> Good, if we don't have enough data, for example...\n",
    "\n",
    "> Think linear vs quadratic fitting and how you decide between those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: QDA and LDA\n",
    "\n",
    "- Use the provided [training](files/Class12-Train.csv) and [query](files/Class12-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n",
    "- Place post-it on your laptop to indicate your status and if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done already?\n",
    "\n",
    "- Visualize the results in the 2D features space\n",
    "- Make these simple codes run faster by getting rid of the 'for' loops, etc.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
