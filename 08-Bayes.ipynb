{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "#Data Mining [EN.550.436]\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "**Class 8** - October 3, 2016\n",
    "\n",
    "- Bayesian inference\n",
    "- Prior: proper vs improper\n",
    "- Likelihood function\n",
    "- Maximum Likelihood Estimation\n",
    "- Links to least squares\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Bayesian Inference</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint & Conditional Probability\n",
    "- Consider random variables $X$, $Y$ of events. Their **joint probability** is\n",
    "\n",
    ">$\\displaystyle P(X, Y) \\neq P(X)\\,P(Y)$ \n",
    ">\n",
    "> instead\n",
    ">\n",
    ">$\\displaystyle P(X, Y) = P(X)\\,P(Y|X)$ \n",
    ">\n",
    "> where $P(Y|X)$ is the **conditional probability** of $Y$ given $X$\n",
    "\n",
    "- For example, if $X$ represents the event of flipping head and $Y$ is tail on the same trial, $P(X,Y)=0$ because $P(Y|X)=0$. \n",
    "\n",
    "- But on separate trials, the events would be independent and we would have $P(Y|X)=P(Y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "- The joint probability of $X$ and $Y$ discrete events\n",
    "\n",
    ">$\\displaystyle P(X,Y) = P(X)\\,P(Y|X)$ \n",
    ">\n",
    "> and \n",
    ">\n",
    ">$\\displaystyle P(Y,X) = P(Y)\\,P(X|Y)$ \n",
    ">\n",
    "> Their equality yields\n",
    ">\n",
    ">$\\displaystyle P(X|Y) = \\frac{P(X)\\,P(Y|X)}{P(Y)}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Densities\n",
    "- It is also true on the continous case and PDFs\n",
    "\n",
    ">$\\displaystyle P(X|y) = \\frac{P(X)\\,p(y|X)}{p(y)}$ \n",
    ">\n",
    "> and\n",
    ">\n",
    ">$\\displaystyle p(x|Y) = \\frac{p(x)\\,P(Y|x)}{P(Y)}$ \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also\n",
    "\n",
    ">$\\displaystyle p(x|y) = \\frac{p(x)\\,p(y|x)}{p(y)}$ \n",
    ">\n",
    "> where\n",
    ">\n",
    ">$\\displaystyle p(y) = \\int p(x)\\,p(y|x)\\,dx$ \n",
    ">\n",
    "> to ensure that\n",
    ">\n",
    ">$\\displaystyle \\int p(x|y)\\,dx = 1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilitistic Model\n",
    "- From data $D$ we can **infer** the parameters $\\theta$ of model $M$ \n",
    "\n",
    ">$\\displaystyle p(\\theta|D) = \\frac{p(\\theta)\\,p(D|\\theta)}{p(D)}$ \n",
    ">\n",
    "> or including the model $M$ explicitly\n",
    ">\n",
    ">$\\displaystyle p(\\theta|D,M) = \\frac{p(\\theta|M)\\,p(D|\\theta,M)}{p(D|M)}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Function\n",
    "- From data $D$ we can **infer** the parameters $\\theta$ of model $M$ \n",
    "\n",
    ">$\\displaystyle p(\\theta|D) = \\frac{\\pi(\\theta)\\,{\\cal{}L}_D(\\theta)}{Z}$ \n",
    ">\n",
    "> where the normalization\n",
    ">\n",
    ">$\\displaystyle Z = \\int \\pi(\\theta)\\,{\\cal{}L}_D(\\theta)\\ d\\theta $ \n",
    "\n",
    "- The **posterior** is proportional to the **prior** times the **likelihood function** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- A set of independent measurements\n",
    "\n",
    ">$\\displaystyle D = \\Big\\{x_i\\Big\\}_{i=1}^N$\n",
    "\n",
    "- E.g., measuring the temperature in $N$ cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameterization\n",
    "\n",
    "- For example, the model is that all cities have the same temperature\n",
    "\n",
    "> We also need to state our prior knowledge about the temperature\n",
    "\n",
    "- Let $\\mu$ represent that temperature in all cities (same for all)\n",
    "\n",
    "> We pick an appropriate prior - often people say we use a \"flat\" prior because we don't know..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Parameterization\n",
    "\n",
    "- We could have chosen another parametrization, say $\\tan^{-1}(\\varphi)$\n",
    "\n",
    "> Clearly a \"flat prior\" means something different!\n",
    "\n",
    "\n",
    "> What should be the prior? Needs careful consideration!\n",
    "\n",
    "- Non-informative prior?\n",
    "\n",
    "> For more, see [Jeffreys](https://en.wikipedia.org/wiki/Harold_Jeffreys) prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the likelihood function?\n",
    "\n",
    "- For a set of independent measurements\n",
    "\n",
    ">$\\displaystyle {\\cal L}_D(\\mu) = p(D|\\mu) = p(\\{x_i\\}|\\mu) = \\prod_{i=1}^N \\ell_{i}(\\mu)$\n",
    "\n",
    "- For example, Gaussian uncertainties\n",
    "\n",
    ">$\\displaystyle \\ell_{i}(\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\ e^{-\\frac{(x_i-\\mu)^2}{2\\sigma_i^2}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Improper Priors\n",
    "\n",
    "- The posterior PDF is\n",
    "\n",
    ">$\\displaystyle p(\\mu|D) = \\frac{\\pi(\\mu) \\prod {\\ell}_i(\\mu)}{\\int \\pi(\\mu) \\prod {\\ell}_i(\\mu)\\,d\\mu}\\ $ \n",
    "\n",
    "- Uniform prior?\n",
    "\n",
    "> Using $\\pi(\\mu)\\!=\\!1$ is clearly wrong but what if the prior is flat over the interval where likelihood function is non-zero (if!), the normalization cancels from the ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n",
    "- Expected value\n",
    "\n",
    ">$\\displaystyle \\int \\mu\\, p(\\mu|D)\\, d\\mu$\n",
    "\n",
    "- Variance: 2nd central moment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "- Maximizing ${\\cal{}L}$ is the same as minimizing $-\\log{\\cal{}L}$ \n",
    "\n",
    "> $\\displaystyle -\\log{\\cal{}L(\\mu)} = \\mathrm{const.} + \\sum_{i=1}^N \\frac{(x_i\\!-\\!\\mu)^2}{2\\sigma_i^2}$\n",
    "\n",
    "> Cf. the method of least squares\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- Weighted average! Using $w_i = 1/\\sigma_i^2$\n",
    "\n",
    "> $\\displaystyle \\hat{\\mu} = \\frac{\\sum w_i x_i}{\\sum w_i}$\n",
    "\n",
    "- Also variance!\n",
    "\n",
    ">$\\displaystyle \\frac{1}{\\sigma_{\\mu}^2} = \\sum w_i = \\sum \\frac{1}{\\sigma_i^2}$\n",
    "\n",
    "> If all have the same $\\sigma$, we have\n",
    "\n",
    ">$\\displaystyle \\frac{1}{\\sigma_{\\mu}^2} = \\frac{N}{\\sigma^2}$\n",
    "$\\ \\ \\ \\rightarrow\\ \\ \\ \\\n",
    "\\displaystyle \\sigma_{\\mu} = {\\sigma}/{\\sqrt{N}}$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
